# Welcome to the YOLOv5 documentation of ECCO project


## How to use this project

!!! info "Information"

    This project rely on the github repo [yolov5 from ultralytics](https://github.com/ultralytics/yolov5), please rely on its [documentation](https://docs.ultralytics.com/) for further explanations about yolov5.


!!! warning "Warning"

    This project has only been tested with Python 3.8.

A Docker image has been provided, see the [Various configuration page](misc_config/docker.md) for further details about the writing of the Dockerfile. The base image from this Dockerfile is a PyTorch image provided by [NVidia](https://ngc.nvidia.com/catalog/containers/nvidia:pytorch/tags), this has the advantage to not bother you with the installation of PyTorch, CUDA, CuDNN, and other optimization softwares needed to make PyTorch compatible with GPUs.

The shell commands needed to build and run the Docker container can be found below, they are also the provided in the [makefile](misc_config/make.md).

```shell
build_docker:
    docker build --rm -f Dockerfile -t yolo_flore_totale:v1 .
```

```shell
run_docker:
    docker run --gpus all --shm-size=2g --ulimit memlock=-1 --ulimit stack=67108864 --mount type=bind,source=$(PWD)/runs,target=/usr/src/app/runs -it --rm -P yolo_flore_totale:v1
```
Note that the container rely on mounting a volume, you can see that in the `run_docker` command : `--mount type=bind,source=$(PWD)/runs,target=/usr/src/app/runs`, this mounted volume is needed to get access to all the results :

* trained models,
* metrics,
* plots,

generated by the training loop.

## Steps to use it

### CRITICAL STEP

**Before anything else**, be sure to have installed [Docker](https://docs.docker.com/get-docker/) and [Nvidia-Container-Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/overview.html).

### Step 0 : Create annotations for an object detection task.

You first need to create labels for your object detection task. This can be done with the use of [makesense.ai](https://www.makesense.ai/), once you have done the labellisation, be sure to export your labels to YOLO format, with one `*.txt` file per image (if no objects in image, no `*.txt` file is required).

The `*.txt` files (generated by Makesense) specifications are:

* One row per object
* Each row is class x_center y_center width height format.
* Box coordinates must be in **normalized xywh** format (from 0 - 1). If your boxes are in pixels, divide `x_center` and `width` by image width, and `y_center` and `height` by image height.
* Class numbers are zero-indexed (start from 0).


!!! example "Exemple"

    Here, the class `0` stands for `Germes`.


    ```
    0 0.19091796875 0.52490234375 0.044921875 0.044921875
    0 0.23974609375 0.53564453125 0.015625 0.01171875
    0 0.26171875 0.5400390625 0.015625 0.0078125
    0 0.271484375 0.56494140625 0.013671875 0.009765625
    0 0.259765625 0.6357421875 0.02734375 0.02734375
    0 0.46728515625 0.6494140625 0.015625 0.0361328125
    0 0.4423828125 0.63671875 0.009765625 0.0185546875
    0 0.4541015625 0.62939453125 0.0126953125 0.017578125
    0 0.39453125 0.64990234375 0.017578125 0.0224609375
    0 0.35302734375 0.6357421875 0.025390625 0.021484375
    0 0.326171875 0.58740234375 0.017578125 0.015625
    0 0.3623046875 0.57568359375 0.0166015625 0.013671875
    0 0.36328125 0.54150390625 0.0146484375 0.0107421875
    0 0.30078125 0.572265625 0.03125 0.013671875
    0 0.3017578125 0.56103515625 0.0341796875 0.013671875
    0 0.26220703125 0.51220703125 0.009765625 0.0068359375
    0 0.23681640625 0.5234375 0.02734375 0.0166015625
    0 0.3037109375 0.525390625 0.0146484375 0.0126953125
    0 0.3388671875 0.5205078125 0.0341796875 0.021484375
    0 0.34326171875 0.5078125 0.02734375 0.0234375
    0 0.28369140625 0.478515625 0.0234375 0.015625
    0 0.2802734375 0.4873046875 0.029296875 0.015625
    ```

### Step 1 : Create train, validation, and test datasets and put the images and annotations files in the right folder

Organize your train, test and val images and labels according to the example below.

![screen](./images/archi1.png)

Each train, test, and valid directory hasve to have the same architecture :

* One directory `images` containing the images,
* One directory `labels` containing the corresponding `*.txt` files.

To inform the scripts where these files are located, you will also have to generate a `data.yaml` file containing the following informations :

```yaml
train: ../train/images
val: ../valid/images

nc: 1
names: ['Germes']
```
This file contains the address where the repositories will be stored when the container will be built, `nc` stands for the number of classes, and `names` for the names of the classes.

!!! info "Information"

    If you only use this project for the designed goal of ECCO, you don't have to modify the `data.yaml` file which is already present in the directory.


### Step 2 : Build the container

Once you've done configuring your train and validation directories, you can build the container with the provided command in the makefile.

```make
make build_docker
```
which is the shortcut for the following command.

```shell
docker build --rm -f Dockerfile -t yolo_flore_totale:v1 .
```

### Step 3 : Run the container


You can run the container with the provided command in the makefile.

```make
make run_docker
```
which is the shortcut for the following command.

```shell
docker run --gpus all --shm-size=2g --ulimit memlock=-1 --ulimit stack=67108864 --mount type=bind,source=$(PWD)/runs,target=/usr/src/app/runs -it --rm -P yolo_flore_totale:v1
```
### Step 4 : Train a model

You can launch the training loop via the following command.

```shell
make train
```
which is the shortcut for the following command.

```shell
python train.py --img 1024 --batch 16 --epochs 3 --data data.yaml --weights yolov5s.pt
```

* `--img` stands for the size of the images, assuming width and height are equal, here we train on images with a resolution $1024 \times 1024$.
* `--batch 16` stands for the batch size.
* `--epochs` stands for the number of epochs, i.e. the total number the entire train dataset pass through the model, the models `v1` and `v2` released have been trained for 300 epochs each.
* `--data` stands for the name of the `*.yaml` defined above, you don't have to modify this argument.
* `--weights` stands for the weights, and architecture, used at the start of the training loop.


### Step 5 : Track your experiments.

The [YOLOv5 image from ultralytics](https://github.com/ultralytics/yolov5) provides an experiment monitoring through [Weights and Biases](https://wandb.ai/site). If you have an account (free accounts are available), you can pass your API key when it will be asked to log your experiment.

### Step 6 : Convert and export your model to ONNX.

```shell
make export_onnx
```
which is the shortcut for the following command.

```shell
python export.py --weights runs/train/exp/weights/best.pt --img 1024 --batch 1
```


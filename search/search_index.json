{"config":{"default_handler":"python","handlers":{"python":{"rendering":{"show_source":true},"setup_commands":["import sys"]}},"indexing":"full","lang":["fr"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the YOLOv5 documentation of ECCO project How to use this project Information This project rely on the github repo yolov5 from ultralytics , please rely on its documentation for further explanations about yolov5. Warning This project has only been tested with Python 3.8. A Docker image has been provided, see the Various configuration page for further details about the writing of the Dockerfile. The base image from this Dockerfile is a PyTorch image provided by NVidia , this has the advantage to not bother you with the installation of PyTorch, CUDA, CuDNN, and other optimization softwares needed to make PyTorch compatible with GPUs. The shell commands needed to build and run the Docker container can be found below, they are also the provided in the makefile . 1 2 build_docker: docker build --rm -f Dockerfile -t yolo_flore_totale:v1 . 1 2 run_docker: docker run --gpus all --shm-size = 2g --ulimit memlock = -1 --ulimit stack = 67108864 --mount type = bind,source = $( PWD ) /runs,target = /usr/src/app/runs -it --rm -P yolo_flore_totale:v1 Note that the container rely on mounting a volume, you can see that in the run_docker command : --mount type=bind,source=$(PWD)/runs,target=/usr/src/app/runs , this mounted volume is needed to get access to all the results : trained models, metrics, plots, generated by the training loop. Steps to use it CRITICAL STEP Before anything else , be sure to have installed Docker and Nvidia-Container-Toolkit . Step 0 : Create annotations for an object detection task. You first need to create labels for your object detection task. This can be done with the use of makesense.ai , once you have done the labellisation, be sure to export your labels to YOLO format, with one *.txt file per image (if no objects in image, no *.txt file is required). The *.txt files (generated by Makesense) specifications are: One row per object Each row is class x_center y_center width height format. Box coordinates must be in normalized xywh format (from 0 - 1). If your boxes are in pixels, divide x_center and width by image width, and y_center and height by image height. Class numbers are zero-indexed (start from 0). Exemple Here, the class 0 stands for Germes . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 0 0.19091796875 0.52490234375 0.044921875 0.044921875 0 0.23974609375 0.53564453125 0.015625 0.01171875 0 0.26171875 0.5400390625 0.015625 0.0078125 0 0.271484375 0.56494140625 0.013671875 0.009765625 0 0.259765625 0.6357421875 0.02734375 0.02734375 0 0.46728515625 0.6494140625 0.015625 0.0361328125 0 0.4423828125 0.63671875 0.009765625 0.0185546875 0 0.4541015625 0.62939453125 0.0126953125 0.017578125 0 0.39453125 0.64990234375 0.017578125 0.0224609375 0 0.35302734375 0.6357421875 0.025390625 0.021484375 0 0.326171875 0.58740234375 0.017578125 0.015625 0 0.3623046875 0.57568359375 0.0166015625 0.013671875 0 0.36328125 0.54150390625 0.0146484375 0.0107421875 0 0.30078125 0.572265625 0.03125 0.013671875 0 0.3017578125 0.56103515625 0.0341796875 0.013671875 0 0.26220703125 0.51220703125 0.009765625 0.0068359375 0 0.23681640625 0.5234375 0.02734375 0.0166015625 0 0.3037109375 0.525390625 0.0146484375 0.0126953125 0 0.3388671875 0.5205078125 0.0341796875 0.021484375 0 0.34326171875 0.5078125 0.02734375 0.0234375 0 0.28369140625 0.478515625 0.0234375 0.015625 0 0.2802734375 0.4873046875 0.029296875 0.015625 Step 1 : Create train, validation, and test datasets and put the images and annotations files in the right folder Organize your train, test and val images and labels according to the example below. Each train, test, and valid directory hasve to have the same architecture : One directory images containing the images, One directory labels containing the corresponding *.txt files. To inform the scripts where these files are located, you will also have to generate a data.yaml file containing the following informations : 1 2 3 4 5 train : ../train/images val : ../valid/images nc : 1 names : [ 'Germes' ] This file contains the address where the repositories will be stored when the container will be built, nc stands for the number of classes, and names for the names of the classes. Information If you only use this project for the designed goal of ECCO, you don't have to modify the data.yaml file which is already present in the directory. Step 2 : Build the container Once you've done configuring your train and validation directories, you can build the container with the provided command in the makefile. 1 make build_docker which is the shortcut for the following command. 1 docker build --rm -f Dockerfile -t yolo_flore_totale:v1 . Step 3 : Run the container You can run the container with the provided command in the makefile. 1 make run_docker which is the shortcut for the following command. 1 docker run --gpus all --shm-size = 2g --ulimit memlock = -1 --ulimit stack = 67108864 --mount type = bind,source = $( PWD ) /runs,target = /usr/src/app/runs -it --rm -P yolo_flore_totale:v1 Step 4 : Train a model You can launch the training loop via the following command. 1 make train which is the shortcut for the following command. 1 python train.py --img 1024 --batch 16 --epochs 3 --data data.yaml --weights yolov5s.pt --img stands for the size of the images, assuming width and height are equal, here we train on images with a resolution \\(1024 \\times 1024\\) . --batch 16 stands for the batch size. --epochs stands for the number of epochs, i.e. the total number the entire train dataset pass through the model, the models v1 and v2 released have been trained for 300 epochs each. --data stands for the name of the *.yaml defined above, you don't have to modify this argument. --weights stands for the weights, and architecture, used at the start of the training loop. Step 5 : Track your experiments. The YOLOv5 image from ultralytics provides an experiment monitoring through Weights and Biases . If you have an account (free accounts are available), you can pass your API key when it will be asked to log your experiment. Step 6 : Convert and export your model to ONNX. 1 make export_onnx which is the shortcut for the following command. 1 python export.py --weights runs/train/exp/weights/best.pt --img 1024 --batch 1","title":"Home"},{"location":"#welcome-to-the-yolov5-documentation-of-ecco-project","text":"","title":"Welcome to the YOLOv5 documentation of ECCO project"},{"location":"#how-to-use-this-project","text":"Information This project rely on the github repo yolov5 from ultralytics , please rely on its documentation for further explanations about yolov5. Warning This project has only been tested with Python 3.8. A Docker image has been provided, see the Various configuration page for further details about the writing of the Dockerfile. The base image from this Dockerfile is a PyTorch image provided by NVidia , this has the advantage to not bother you with the installation of PyTorch, CUDA, CuDNN, and other optimization softwares needed to make PyTorch compatible with GPUs. The shell commands needed to build and run the Docker container can be found below, they are also the provided in the makefile . 1 2 build_docker: docker build --rm -f Dockerfile -t yolo_flore_totale:v1 . 1 2 run_docker: docker run --gpus all --shm-size = 2g --ulimit memlock = -1 --ulimit stack = 67108864 --mount type = bind,source = $( PWD ) /runs,target = /usr/src/app/runs -it --rm -P yolo_flore_totale:v1 Note that the container rely on mounting a volume, you can see that in the run_docker command : --mount type=bind,source=$(PWD)/runs,target=/usr/src/app/runs , this mounted volume is needed to get access to all the results : trained models, metrics, plots, generated by the training loop.","title":"How to use this project"},{"location":"#steps-to-use-it","text":"","title":"Steps to use it"},{"location":"#critical-step","text":"Before anything else , be sure to have installed Docker and Nvidia-Container-Toolkit .","title":"CRITICAL STEP"},{"location":"#step-0-create-annotations-for-an-object-detection-task","text":"You first need to create labels for your object detection task. This can be done with the use of makesense.ai , once you have done the labellisation, be sure to export your labels to YOLO format, with one *.txt file per image (if no objects in image, no *.txt file is required). The *.txt files (generated by Makesense) specifications are: One row per object Each row is class x_center y_center width height format. Box coordinates must be in normalized xywh format (from 0 - 1). If your boxes are in pixels, divide x_center and width by image width, and y_center and height by image height. Class numbers are zero-indexed (start from 0). Exemple Here, the class 0 stands for Germes . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 0 0.19091796875 0.52490234375 0.044921875 0.044921875 0 0.23974609375 0.53564453125 0.015625 0.01171875 0 0.26171875 0.5400390625 0.015625 0.0078125 0 0.271484375 0.56494140625 0.013671875 0.009765625 0 0.259765625 0.6357421875 0.02734375 0.02734375 0 0.46728515625 0.6494140625 0.015625 0.0361328125 0 0.4423828125 0.63671875 0.009765625 0.0185546875 0 0.4541015625 0.62939453125 0.0126953125 0.017578125 0 0.39453125 0.64990234375 0.017578125 0.0224609375 0 0.35302734375 0.6357421875 0.025390625 0.021484375 0 0.326171875 0.58740234375 0.017578125 0.015625 0 0.3623046875 0.57568359375 0.0166015625 0.013671875 0 0.36328125 0.54150390625 0.0146484375 0.0107421875 0 0.30078125 0.572265625 0.03125 0.013671875 0 0.3017578125 0.56103515625 0.0341796875 0.013671875 0 0.26220703125 0.51220703125 0.009765625 0.0068359375 0 0.23681640625 0.5234375 0.02734375 0.0166015625 0 0.3037109375 0.525390625 0.0146484375 0.0126953125 0 0.3388671875 0.5205078125 0.0341796875 0.021484375 0 0.34326171875 0.5078125 0.02734375 0.0234375 0 0.28369140625 0.478515625 0.0234375 0.015625 0 0.2802734375 0.4873046875 0.029296875 0.015625","title":"Step 0 : Create annotations for an object detection task."},{"location":"#step-1-create-train-validation-and-test-datasets-and-put-the-images-and-annotations-files-in-the-right-folder","text":"Organize your train, test and val images and labels according to the example below. Each train, test, and valid directory hasve to have the same architecture : One directory images containing the images, One directory labels containing the corresponding *.txt files. To inform the scripts where these files are located, you will also have to generate a data.yaml file containing the following informations : 1 2 3 4 5 train : ../train/images val : ../valid/images nc : 1 names : [ 'Germes' ] This file contains the address where the repositories will be stored when the container will be built, nc stands for the number of classes, and names for the names of the classes. Information If you only use this project for the designed goal of ECCO, you don't have to modify the data.yaml file which is already present in the directory.","title":"Step 1 : Create train, validation, and test datasets and put the images and annotations files in the right folder"},{"location":"#step-2-build-the-container","text":"Once you've done configuring your train and validation directories, you can build the container with the provided command in the makefile. 1 make build_docker which is the shortcut for the following command. 1 docker build --rm -f Dockerfile -t yolo_flore_totale:v1 .","title":"Step 2 : Build the container"},{"location":"#step-3-run-the-container","text":"You can run the container with the provided command in the makefile. 1 make run_docker which is the shortcut for the following command. 1 docker run --gpus all --shm-size = 2g --ulimit memlock = -1 --ulimit stack = 67108864 --mount type = bind,source = $( PWD ) /runs,target = /usr/src/app/runs -it --rm -P yolo_flore_totale:v1","title":"Step 3 : Run the container"},{"location":"#step-4-train-a-model","text":"You can launch the training loop via the following command. 1 make train which is the shortcut for the following command. 1 python train.py --img 1024 --batch 16 --epochs 3 --data data.yaml --weights yolov5s.pt --img stands for the size of the images, assuming width and height are equal, here we train on images with a resolution \\(1024 \\times 1024\\) . --batch 16 stands for the batch size. --epochs stands for the number of epochs, i.e. the total number the entire train dataset pass through the model, the models v1 and v2 released have been trained for 300 epochs each. --data stands for the name of the *.yaml defined above, you don't have to modify this argument. --weights stands for the weights, and architecture, used at the start of the training loop.","title":"Step 4 : Train a model"},{"location":"#step-5-track-your-experiments","text":"The YOLOv5 image from ultralytics provides an experiment monitoring through Weights and Biases . If you have an account (free accounts are available), you can pass your API key when it will be asked to log your experiment.","title":"Step 5 : Track your experiments."},{"location":"#step-6-convert-and-export-your-model-to-onnx","text":"1 make export_onnx which is the shortcut for the following command. 1 python export.py --weights runs/train/exp/weights/best.pt --img 1024 --batch 1","title":"Step 6 : Convert and export your model to ONNX."},{"location":"misc_config/docker/","text":"Dockerfile configuration 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 # YOLOv5 \ud83d\ude80 by Ultralytics, GPL-3.0 license # Start FROM Nvidia PyTorch image https://ngc.nvidia.com/catalog/containers/nvidia:pytorch FROM nvcr.io/nvidia/pytorch:21.05-py3 # Install linux packages RUN apt update && apt install -y zip htop screen libgl1-mesa-glx # Install python dependencies COPY requirements.txt . RUN python -m pip install --upgrade pip RUN pip uninstall -y nvidia-tensorboard nvidia-tensorboard-plugin-dlprof RUN pip install --no-cache -r requirements.txt coremltools onnx gsutil notebook wandb> = 0 .12.2 # Create working directory RUN mkdir -p /usr/src/app WORKDIR /usr/src/app # Copy contents COPY . /usr/src/app # Downloads to user config dir ADD https://ultralytics.com/assets/Arial.ttf /root/.config/Ultralytics/ RUN mv ./test ../test RUN mv ./train ../train RUN mv ./valid ../valid RUN mv data.yaml ./data/","title":"Dockerfile"},{"location":"misc_config/docker/#dockerfile-configuration","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 # YOLOv5 \ud83d\ude80 by Ultralytics, GPL-3.0 license # Start FROM Nvidia PyTorch image https://ngc.nvidia.com/catalog/containers/nvidia:pytorch FROM nvcr.io/nvidia/pytorch:21.05-py3 # Install linux packages RUN apt update && apt install -y zip htop screen libgl1-mesa-glx # Install python dependencies COPY requirements.txt . RUN python -m pip install --upgrade pip RUN pip uninstall -y nvidia-tensorboard nvidia-tensorboard-plugin-dlprof RUN pip install --no-cache -r requirements.txt coremltools onnx gsutil notebook wandb> = 0 .12.2 # Create working directory RUN mkdir -p /usr/src/app WORKDIR /usr/src/app # Copy contents COPY . /usr/src/app # Downloads to user config dir ADD https://ultralytics.com/assets/Arial.ttf /root/.config/Ultralytics/ RUN mv ./test ../test RUN mv ./train ../train RUN mv ./valid ../valid RUN mv data.yaml ./data/","title":"Dockerfile configuration"},{"location":"misc_config/make/","text":"Makefile configuration 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 # Makefile .PHONY : build_docker build_docker : docker build \\ --rm -f Dockerfile \\ -t yolo_flore_totale:v1 . .PHONY : run_docker run_docker : docker run \\ --gpus all \\ --shm-size = 2g \\ --ulimit memlock = -1 \\ --ulimit stack = 67108864 \\ --mount type = bind,source = $( PWD ) /runs,target = /usr/src/app/runs \\ -it --rm -P yolo_flore_totale:v1 .PHONY : train train : python train.py \\ --img 1024 \\ --batch 16 \\ --epochs 3 \\ --data data.yaml \\ --weights yolov5s.pt .PHONY : export_onnx export_onnx : python export.py \\ --weights runs/train/exp/weights/best.pt \\ --img 1024 \\ --batch 1 .PHONY : docs docs : mkdocs serve .PHONY : site site : mkdocs build .PHONY : install - docs install-docs : pip install -r requirements-doc.txt --no-cache-dir","title":"Makefile"},{"location":"misc_config/make/#makefile-configuration","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 # Makefile .PHONY : build_docker build_docker : docker build \\ --rm -f Dockerfile \\ -t yolo_flore_totale:v1 . .PHONY : run_docker run_docker : docker run \\ --gpus all \\ --shm-size = 2g \\ --ulimit memlock = -1 \\ --ulimit stack = 67108864 \\ --mount type = bind,source = $( PWD ) /runs,target = /usr/src/app/runs \\ -it --rm -P yolo_flore_totale:v1 .PHONY : train train : python train.py \\ --img 1024 \\ --batch 16 \\ --epochs 3 \\ --data data.yaml \\ --weights yolov5s.pt .PHONY : export_onnx export_onnx : python export.py \\ --weights runs/train/exp/weights/best.pt \\ --img 1024 \\ --batch 1 .PHONY : docs docs : mkdocs serve .PHONY : site site : mkdocs build .PHONY : install - docs install-docs : pip install -r requirements-doc.txt --no-cache-dir","title":"Makefile configuration"}]}